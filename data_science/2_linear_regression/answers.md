- What is the advantage of using sequential learning for fitting linear model over the analytical solution?
  - What the fuck is sequential learning?
- What is the geometrical meaning of the least square solution to linear regression?
  - This was not mentioned in book (the word geometric only shows up twice in the book, in the PCA chapter).
- Why does L1 regularization tend to make coefficients vanish exactly, while L2 doesnâ€™t?
  - L1 tends to make coefficients since it tends to decrease the absolute values of all coefficients equally, 
    whereas L2 tends to scale the absolute values instead. Therefore, L2 tends to make small coefficients smaller, 
    whereas L1 just zeroes them out.
